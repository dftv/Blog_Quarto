@misc{WhatAreAIHallucinations,
  author       = {IBM},
  howpublished = {Available at \url{https://www.ibm.com/think/topics/ai-hallucinations}},
  title        = {What are AI hallucinations?},
  year         = {2023},
  note         = {Accessed: Feb. 8, 2025}
}

@misc{WhatIsAContextWindow,
  author       = {Dave Bergmann},
  howpublished = {Available at: \url{https://www.ibm.com/think/topics/context-window}},
  title        = {What is a context window?},
  year         = {2024},
  note         = {Accessed: Feb. 8, 2025}
}

@misc{UnderstandingPsychologicalReactance,
  author    = {Steindl, Christina and Jonas, Eva and Sittenthaler, Sandra and Traut-Mattausch, Eva and Greenberg, Jeff},
  title     = {Understanding Psychological Reactance: New Developments and FindingsUnderstanding Psychological Reactance: New Developments and Findings},
  year      = {2015},
  publisher = {Hogrefe Publishing},
  number    = {4},
  issn      = {2190-8370},
  url       = {https://doi.org/10.1027/2151-2604/a000222},
  note      = {27453805},
  pages     = {205–214},
  numpages  = {214}
}

@misc{HowDoesReversePsychologyWork,
  author       = {Kendra Cherry},
  howpublished = {Available at: \url{https://www.verywellmind.com/what-is-reverse-psychology-5115635}},
  title        = {How Does Reverse Psychology Work?},
  year         = {2023}
}

@article{DoesAutomationBiasDecisionMaking,
  title    = {Does automation bias decision-making?},
  journal  = {International Journal of Human-Computer Studies},
  volume   = {51},
  number   = {5},
  pages    = {991-1006},
  year     = {1999},
  issn     = {1071-5819},
  doi      = {https://doi.org/10.1006/ijhc.1999.0252},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581999902525},
  author   = {LINDA J. SKITKA and KATHLEEN L. MOSIER and MARK BURDICK},
  abstract = {Computerized system monitors and decision aids are increasingly common additions to critical decision-making contexts such as intensive care units, nuclear power plants and aircraft cockpits. These aids are introduced with the ubiquitous goal of “reducing human error”. The present study compared error rates in a simulated flight task with and without a computer that monitored system states and made decision recommendations. Participants in non-automated settings out-performed their counterparts with a very but not perfectly reliable automated aid on a monitoring task. Participants with an aid made errors of omission (missed events when not explicitly prompted about them by the aid) and commission (did what an automated aid recommended, even when it contradicted their training and other 100% valid and available indicators). Possible causes and consequences of automation bias are discussed}
}